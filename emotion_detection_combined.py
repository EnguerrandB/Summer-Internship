# -*- coding: utf-8 -*-
"""Emotion Detection Combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qBDvNWKcx9O2lmAXMO4MLnPPVqdd8x06

#**IMPORTS**

### All
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pydub
# !pip install python_speech_features
# !pip install SpeechRecognition
# !pip install imageio==2.4.1
# # !pip uninstall audio
# 
# 
# # !pip install SoundFile
# # !pip install videoclip
# # !pip install scipy==1.1.0
# 
# 
# 
# 
# from tqdm import tqdm
# import numpy as np 
# import pandas as pd 
# import pickle
# import os
# import matplotlib.pyplot as plt
# import scipy
# import keras
# from sklearn.model_selection import train_test_split
# from keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Input, BatchNormalization, MaxPooling2D, Activation, Flatten, Dense, Dropout
# from keras.models import Model,Sequential
# from tensorflow.keras.utils import to_categorical
# from sklearn.metrics import classification_report
# from imblearn.over_sampling import RandomOverSampler
# from keras.preprocessing import image
# import tensorflow as tf
# import cv2
# from pydub import AudioSegment 
# import logging
# import sys
# from keras.regularizers import l2
# from keras import backend as K
# from scipy.io import loadmat
# from random import shuffle
# import moviepy.editor as mp
# 
# 
# # from scipy.misc import imread, imresize
# from statistics import mode
# from keras.models import load_model
# from google.colab.patches import cv2_imshow
# import tensorflow as tf
# from tensorflow.keras.layers import Input, Dense
# 
# import soundfile
# from python_speech_features import mfcc , logfbank
# from glob import glob
# from sklearn.neural_network import MLPClassifier
# import librosa
# from moviepy.editor import *
# import speech_recognition as sr
# 
# # import audio
# # import os
# # import glob
# # from tqdm import tqdm
# # import pandas as pd
# # import numpy as np
# # import matplotlib.pyplot as plt
# # from scipy.io import wavfile
# # from python_speech_features import mfcc , logfbank
# # import librosa as lr
# # import os, glob, pickle
# # from scipy import signal
# # import noisereduce as nr
# # from glob import glob
# # import tensorflow as tf
# # import librosa
# # get_ipython().magic('matplotlib inline')
# # import soundfile
# # from tensorflow.keras.layers import Conv2D,MaxPool2D, Flatten, LSTM
# # from keras.layers import Dropout,Dense,TimeDistributed
# # from keras.models import Sequential
# # from tensorflow.keras.utils import to_categorical
# # from sklearn.utils.class_weight import compute_class_weight
# # from sklearn.model_selection import train_test_split
# # from sklearn.neural_network import MLPClassifier
# # from sklearn.metrics import accuracy_score

"""##Drive"""

from google.colab import drive
drive.mount('/content/drive')
!ls

for dirname, _, filenames in os.walk('data'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


os.chdir("./drive/MyDrive/Adults databases/Trained_Models/")

"""##Transformer"""

#for deBERTa
!pip install transformers
from transformers import AutoTokenizer,TFDebertaModel
deberta = TFDebertaModel.from_pretrained('microsoft/deberta-base')
tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')

#for all the rest
from tensorflow.keras.optimizers import Adam,SGD
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.initializers import TruncatedNormal
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications import ResNet50, ResNet50V2



"""#UTILS

###Text - Speech2Text
"""

#Speech 2 text

def speech_to_text(filename):
  r = sr.Recognizer()
  speech = ""

  audio_file=sr.AudioFile(filename)
  with audio_file as source:
      audio = r.record(source)
  try:
      speech = r.recognize_google(audio)
      print("Text recognised: "+speech)
  except Exception as e:
      print("Exception: "+str(e))
  return speech

# speech_to_text("example anger.mov")

"""### ResNet label utils """

#label utils
class DataManager(object):
    def __init__(self, dataset_name='imdb',
                 dataset_path=None, image_size=(48, 48)):

        self.dataset_name = dataset_name
        self.dataset_path = dataset_path
        self.image_size = image_size
        if self.dataset_path is not None:
            self.dataset_path = dataset_path
        elif self.dataset_name == 'imdb':
            self.dataset_path = '../datasets/imdb_crop/imdb.mat'
        elif self.dataset_name == 'fer2013':
            self.dataset_path = '../datasets/fer2013/fer2013.csv'
        elif self.dataset_name == 'KDEF':
            self.dataset_path = '../datasets/KDEF/'
        else:
            raise Exception(
                    'Incorrect dataset name, please input imdb or fer2013')

    def get_data(self):
        if self.dataset_name == 'imdb':
            ground_truth_data = self._load_imdb()
        elif self.dataset_name == 'fer2013':
            ground_truth_data = self._load_fer2013()
        elif self.dataset_name == 'KDEF':
            ground_truth_data = self._load_KDEF()
        return ground_truth_data

    def _load_imdb(self):
        face_score_treshold = 3
        dataset = loadmat(self.dataset_path)
        image_names_array = dataset['imdb']['full_path'][0, 0][0]
        gender_classes = dataset['imdb']['gender'][0, 0][0]
        face_score = dataset['imdb']['face_score'][0, 0][0]
        second_face_score = dataset['imdb']['second_face_score'][0, 0][0]
        face_score_mask = face_score > face_score_treshold
        second_face_score_mask = np.isnan(second_face_score)
        unknown_gender_mask = np.logical_not(np.isnan(gender_classes))
        mask = np.logical_and(face_score_mask, second_face_score_mask)
        mask = np.logical_and(mask, unknown_gender_mask)
        image_names_array = image_names_array[mask]
        gender_classes = gender_classes[mask].tolist()
        image_names = []
        for image_name_arg in range(image_names_array.shape[0]):
            image_name = image_names_array[image_name_arg][0]
            image_names.append(image_name)
        return dict(zip(image_names, gender_classes))

    def _load_fer2013(self):
        data = pd.read_csv(self.dataset_path)
        pixels = data['pixels'].tolist()
        width, height = 48, 48
        faces = []
        for pixel_sequence in pixels:
            face = [int(pixel) for pixel in pixel_sequence.split(' ')]
            face = np.asarray(face).reshape(width, height)
            face = cv2.resize(face.astype('uint8'), self.image_size)
            faces.append(face.astype('float32'))
        faces = np.asarray(faces)
        faces = np.expand_dims(faces, -1)
        emotions = pd.get_dummies(data['emotion']).as_matrix()
        return faces, emotions

    def _load_KDEF(self):
        class_to_arg = get_class_to_arg(self.dataset_name)
        num_classes = len(class_to_arg)

        file_paths = []
        for folder, subfolders, filenames in os.walk(self.dataset_path):
            for filename in filenames:
                if filename.lower().endswith(('.jpg')):
                    file_paths.append(os.path.join(folder, filename))

        num_faces = len(file_paths)
        y_size, x_size = self.image_size
        faces = np.zeros(shape=(num_faces, y_size, x_size))
        emotions = np.zeros(shape=(num_faces, num_classes))
        for file_arg, file_path in enumerate(file_paths):
            image_array = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
            image_array = cv2.resize(image_array, (y_size, x_size))
            faces[file_arg] = image_array
            file_basename = os.path.basename(file_path)
            file_emotion = file_basename[4:6]
            # there are two file names in the dataset
            # that don't match the given classes
            try:
                emotion_arg = class_to_arg[file_emotion]
            except:
                continue
            emotions[file_arg, emotion_arg] = 1
        faces = np.expand_dims(faces, -1)
        return faces, emotions


def get_labels(dataset_name):
    if dataset_name == 'fer2013':
        return {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy',
                4: 'sad', 5: 'surprise', 6: 'neutral'}
    elif dataset_name == 'imdb':
        return {0: 'woman', 1: 'man'}
    elif dataset_name == 'KDEF':
        return {0: 'AN', 1: 'DI', 2: 'AF', 3: 'HA', 4: 'SA', 5: 'SU', 6: 'NE'}
    else:
        raise Exception('Invalid dataset name')


def get_class_to_arg(dataset_name='fer2013'):
    if dataset_name == 'fer2013':
        return {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'sad': 4,
                'surprise': 5, 'neutral': 6}
    elif dataset_name == 'imdb':
        return {'woman': 0, 'man': 1}
    elif dataset_name == 'KDEF':
        return {'AN': 0, 'DI': 1, 'AF': 2, 'HA': 3, 'SA': 4, 'SU': 5, 'NE': 6}
    else:
        raise Exception('Invalid dataset name')


def split_imdb_data(ground_truth_data, validation_split=.2, do_shuffle=False):
    ground_truth_keys = sorted(ground_truth_data.keys())
    if do_shuffle is not False:
        shuffle(ground_truth_keys)
    training_split = 1 - validation_split
    num_train = int(training_split * len(ground_truth_keys))
    train_keys = ground_truth_keys[:num_train]
    validation_keys = ground_truth_keys[num_train:]
    return train_keys, validation_keys


def split_data(x, y, validation_split=.2):
    num_samples = len(x)
    num_train_samples = int((1 - validation_split)*num_samples)
    train_x = x[:num_train_samples]
    train_y = y[:num_train_samples]
    val_x = x[num_train_samples:]
    val_y = y[num_train_samples:]
    train_data = (train_x, train_y)
    val_data = (val_x, val_y)
    return train_data, val_data

"""###ResNet interference utils

"""

###Resnet interference utils


def load_image(image_path, grayscale=False, target_size=None):
    pil_image = image.load_img(image_path, grayscale, target_size)
    return image.img_to_array(pil_image)

def load_detection_model(model_path):
    detection_model = cv2.CascadeClassifier(model_path)
    return detection_model

def detect_faces(detection_model, gray_image_array):
    return detection_model.detectMultiScale(gray_image_array, 1.3, 2)

def draw_bounding_box(face_coordinates, image_array, color):
    x, y, w, h = face_coordinates
    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)

def apply_offsets(face_coordinates, offsets):
    x, y, width, height = face_coordinates
    x_off, y_off = offsets
    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)

def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,
                                                font_scale=2, thickness=2):
    x, y = coordinates[:2]
    cv2.putText(image_array, text, (x + x_offset, y + y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                font_scale, color, thickness, cv2.LINE_AA)

def get_colors(num_classes):
    colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()
    colors = np.asarray(colors) * 255
    return colors

"""###Resnet preprocess utils"""

#Resnet preprocess utils

def preprocess_input(x, v2=True):
    x = x.astype('float32')
    x = x / 255.0
    if v2:
        x = x - 0.5
        x = x * 2.0
    return x


def _imread(image_name):
        return imread(image_name)


def _imresize(image_array, size):
        return imresize(image_array, size)


def to_categorical(integer_classes, num_classes=2):
    integer_classes = np.asarray(integer_classes, dtype='int')
    num_samples = integer_classes.shape[0]
    categorical = np.zeros((num_samples, num_classes))
    categorical[np.arange(num_samples), integer_classes] = 1
    return categorical

"""##ResNet Draw Graph"""

def drawGraphImage(emotionArray):
  COLOR = "black"

  plt.rcParams["figure.figsize"] = [7.50, 3.50]
  plt.rcParams["figure.autolayout"] = True

  plt.rcParams['text.color'] = COLOR
  plt.rcParams['axes.labelcolor'] = COLOR
  plt.rcParams['xtick.color'] = COLOR
  plt.rcParams['ytick.color'] = COLOR

  plt.ylim(-0.15,1)

  x1 = np.array(emotionArray[0])
  x2 = np.array(emotionArray[1])
  x3 = np.array(emotionArray[2])
  x4 = np.array(emotionArray[3])
  x5 = np.array(emotionArray[4])
  x6 = np.array(emotionArray[5])
  x7 = np.array(emotionArray[6])
  #y = (0, totalframecount)

  plt.plot(x1, color="red",label="Anger")
  plt.plot(x2, color="black",label="Disgust")
  plt.plot(x3, color="purple",label="Fear")
  plt.plot(x4, color="green",label="Happiness")
  plt.plot(x5, color="blue",label="Sadness")
  plt.plot(x6, color="orange",label="Surprise")
  plt.plot(x7, color="grey",label="Neutral")

  plt.xlabel('Frames')
  plt.ylabel('Accuracy (%)')


  plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

  plt.savefig('hello.pdf')

  plt.show()

"""##**Audio Preprocess**"""



"""## **Audio Features retrieval**"""

# #Feature Extraction of Audio Files Function 
def extract_feature(audio, mfcc, chroma, mel):
    #with soundfile.SoundFile(file_name) as sound_file:

    X , sample_rate = librosa.load(audio)
    # X = audio.read(dtype="float32")

    # sample_rate=audio.samplerate
    if chroma:
        stft=np.abs(librosa.stft(X))
    result=np.array([])
    # print("step 1 : ",result.shape)
    if mfcc:
        # print(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).shape)
        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
    result=np.hstack((result, mfccs))
    # print("step 2 : ",result.shape)
    if chroma:
        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
    result=np.hstack((result, chroma))
    # print("step 3 : ",result.shape)
    if mel:
        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)
    result=np.hstack((result, mel))
    # print("step 4 : ",result.shape)
    return result

"""#**MODELS**

##TEXT - Deberta
"""

def textPredict(text):

  max_len = 68
  import tensorflow as tf
  from tensorflow.keras.layers import Input, Dense

  input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
  input_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")
  # embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]


  embeddings = deberta(input_ids,attention_mask = input_mask)[0] 
  out = tf.keras.layers.GlobalMaxPool1D()(embeddings)
  out = Dense(128, activation='relu')(out)
  out = tf.keras.layers.Dropout(0.1)(out)
  out = Dense(32,activation = 'relu')(out)

  y = Dense(7,activation = 'sigmoid')(out)
      
  model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)
  model.layers[2].trainable = True
  # for training bert our lr must be so small

  loadedModel = model.load_weights('deberta_95.h5')

  x_val = tokenizer(
  text=text,
  add_special_tokens=True,
  max_length=68,
  truncation=True,
  padding='max_length', 
  return_tensors='tf',
  return_token_type_ids = False,
  return_attention_mask = True,
  verbose = True)
  
  
  return(model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']}))

print(textPredict("I despise you"))

list_emotions_text = ["Anger", "Disgust", "Fear", "Happiness", "Neutral", "Sadness", "Surprise"]

# print(textPredict("I detest faults in soccer"))

# list_emotions_text = ["Anger", "Disgust", "Fear", "Happiness", "Neutral", "Sadness", "Surprise"]

"""##IMAGE - ResNet 50"""

# !ls ./drive/MyDrive/Adults\ databases/Trained_Models/fer2013_mini_XCEPTION.119-0.65.hdf5

# parameters for loading data and images
detection_model_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
emotion_model_path = 'fer2013_mini_XCEPTION.119-0.65.hdf5'

 

emotion_labels = get_labels('fer2013')

# hyper-parameters for bounding boxes shape
frame_window = 10
emotion_offsets = (20, 40)

# loading models
face_detection = load_detection_model(detection_model_path)
emotion_classifier = load_model(emotion_model_path, compile=False)

# getting input model shapes for inference
emotion_target_size = emotion_classifier.input_shape[1:3]

# starting lists for calculating modes
emotion_window = []

# starting video streaming
# cv2.namedWindow('window_frame')

"""#**FUNCTIONS**

##**Deberta**
"""

# def textPredict(text):

#   return getModel(text)

"""##**CNN**"""

def audioPredict(filename):
  filename_audio = mp4_to_wav(filename)
  

  # Extract features
  audio_features = extract_feature(filename_audio,mfcc=True,mel=True,chroma=True)
  audio_features = audio_features[np.newaxis,:]

  # Load the Model back from file
  Pkl_Filename = "models/Emotion_Voice_Detection_Model.pkl"  
  with open(Pkl_Filename, 'rb') as file:  
      Emotion_Voice_Detection_Model = pickle.load(file)

  #prediction
  emotions_predicted = Emotion_Voice_Detection_Model.predict_proba(audio_features) #verify if its an array (%)
  # print(Emotion_Voice_Detection_Model.predict(audio_features))


  #return array of predictions
  return emotions_predicted

def mp4_to_wav(file):
    audio = AudioSegment.from_file(file, format="mp4")
    audio = stereo_to_mono(audio)
    output = "audio/" + file
    audio.export(output, format="wav")
    return output

def stereo_to_mono(audio_segment):
  audio_segment = audio_segment.set_channels(1)
  return audio_segment

"""##**ResNet 50**"""

def imagePredict(video_filename, drawGraph):
    anger = []
    disgust = []
    fear = []
    happiness = []
    sadness = []
    surprise = []
    neutral = []

    # To capture video from existing video.   
    cap = cv2.VideoCapture(video_filename)  

    totalframecount= int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 2

    # print("total frame count : " , totalframecount)

    i = 0


    emotion_array = []

    for i in tqdm(range(totalframecount)):

      #print("image : ", i)
      _, bgr_image = cap.read()


    # video_capture = cv2.VideoCapture(0)
    # while True:
    #     bgr_image = video_capture.read()[1]
      try:
        gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)
      
        rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
        faces = detect_faces(face_detection, gray_image)

        for face_coordinates in faces:

            x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)
            gray_face = gray_image[y1:y2, x1:x2]
            try:
                gray_face = cv2.resize(gray_face, (emotion_target_size))
            except:
                continue

            gray_face = preprocess_input(gray_face, True)
            gray_face = np.expand_dims(gray_face, 0)
            gray_face = np.expand_dims(gray_face, -1)
            emotion_prediction = emotion_classifier.predict(gray_face)
            emotion_probability = np.max(emotion_prediction)
            emotion_label_arg = np.argmax(emotion_prediction)
            emotion_text = emotion_labels[emotion_label_arg]
            emotion_window.append(emotion_text)

            
            anger.append(emotion_prediction[0][0])
            disgust.append(emotion_prediction[0][1])
            fear.append(emotion_prediction[0][2])
            happiness.append(emotion_prediction[0][3])
            sadness.append(emotion_prediction[0][4])
            surprise.append(emotion_prediction[0][5])
            neutral.append(emotion_prediction[0][6])


            if len(emotion_window) > frame_window:
                emotion_window.pop(0)
            try:
                emotion_mode = mode(emotion_window)
            except:
                continue

            if emotion_text == 'angry':
                color = emotion_probability * np.asarray((255, 0, 0))
            elif emotion_text == 'sad':
                color = emotion_probability * np.asarray((0, 0, 255))
            elif emotion_text == 'happy':
                color = emotion_probability * np.asarray((255, 255, 0))
            elif emotion_text == 'surprise':
                color = emotion_probability * np.asarray((0, 255, 255))
            elif emotion_text == 'disgust':
                color = emotion_probability * np.asarray((0, 127, 255))
            elif emotion_text == 'fear':
                color = emotion_probability * np.asarray((0, 127, 127))
            else:
                color = emotion_probability * np.asarray((0, 255, 0))

            color = color.astype(int)
            color = color.tolist()

            draw_bounding_box(face_coordinates, rgb_image, color)
            draw_text(face_coordinates, rgb_image, emotion_mode,color, 0, -45, 1, 1)

        

      except:
        continue   
        
        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
        cv2_imshow(bgr_image)


    list_emotions = ["Angry", "Disgust", "Fear", "Happiness", "Sad", "Surprise", "Neutral"]
    emotion = []

    emotion.append(anger)
    emotion.append(disgust)
    emotion.append(fear)
    emotion.append(happiness)
    emotion.append(sadness)
    emotion.append(surprise)
    emotion.append(neutral)
    # print("here : " ,emotion)

    if drawGraph:
      drawGraphImage(emotion)

    return [np.mean(emotion[0]), np.mean(emotion[1]), np.mean(emotion[2]), np.mean(emotion[3]), np.mean(emotion[4]), np.mean(emotion[5]), np.mean(emotion[6])]

"""##**3D CNN**"""

def motionPredict(video_filename, drawGraph):
    anger = []
    disgust = []
    fear = []
    happiness = []
    sadness = []
    surprise = []
    neutral = []

    # To capture video from existing video.   
    cap = cv2.VideoCapture(video_filename)  

    totalframecount= int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 2

    # print("total frame count : " , totalframecount)

    i = 0


    emotion_array = []

    for i in tqdm(range(totalframecount)):

      #print("image : ", i)
      _, bgr_image = cap.read()


    # video_capture = cv2.VideoCapture(0)
    # while True:
    #     bgr_image = video_capture.read()[1]
      try:
        gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)
      
        rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
        faces = detect_faces(face_detection, gray_image)

        for face_coordinates in faces:

            x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)
            gray_face = gray_image[y1:y2, x1:x2]
            try:
                gray_face = cv2.resize(gray_face, (emotion_target_size))
            except:
                continue

            gray_face = preprocess_input(gray_face, True)
            gray_face = np.expand_dims(gray_face, 0)
            gray_face = np.expand_dims(gray_face, -1)
            emotion_prediction = emotion_classifier.predict(gray_face)
            emotion_probability = np.max(emotion_prediction)
            emotion_label_arg = np.argmax(emotion_prediction)
            emotion_text = emotion_labels[emotion_label_arg]
            emotion_window.append(emotion_text)

            
            anger.append(emotion_prediction[0][0])
            disgust.append(emotion_prediction[0][1])
            fear.append(emotion_prediction[0][2])
            happiness.append(emotion_prediction[0][3])
            sadness.append(emotion_prediction[0][4])
            surprise.append(emotion_prediction[0][5])
            neutral.append(emotion_prediction[0][6])


            if len(emotion_window) > frame_window:
                emotion_window.pop(0)
            try:
                emotion_mode = mode(emotion_window)
            except:
                continue

            if emotion_text == 'angry':
                color = emotion_probability * np.asarray((255, 0, 0))
            elif emotion_text == 'sad':
                color = emotion_probability * np.asarray((0, 0, 255))
            elif emotion_text == 'happy':
                color = emotion_probability * np.asarray((255, 255, 0))
            elif emotion_text == 'surprise':
                color = emotion_probability * np.asarray((0, 255, 255))
            elif emotion_text == 'disgust':
                color = emotion_probability * np.asarray((0, 127, 255))
            elif emotion_text == 'fear':
                color = emotion_probability * np.asarray((0, 127, 127))
            else:
                color = emotion_probability * np.asarray((0, 255, 0))

            color = color.astype(int)
            color = color.tolist()

            draw_bounding_box(face_coordinates, rgb_image, color)
            draw_text(face_coordinates, rgb_image, emotion_mode,color, 0, -45, 1, 1)

        

      except:
        continue   
        
        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
        cv2_imshow(bgr_image)


    list_emotions = ["Angry", "Disgust", "Fear", "Happiness", "Sad", "Surprise", "Neutral"]
    emotion = []

    emotion.append(anger)
    emotion.append(disgust)
    emotion.append(fear)
    emotion.append(happiness)
    emotion.append(sadness)
    emotion.append(surprise)
    emotion.append(neutral)

    if drawGraph:
      drawGraphImage(emotion)

    return [np.mean(emotion[0]), np.mean(emotion[1]), np.mean(emotion[2]), np.mean(emotion[3]), np.mean(emotion[4]), np.mean(emotion[5]), np.mean(emotion[6])]

"""#**MAIN**

###Organise emotions array
"""

list_emotions_image = ["Anger", "Disgust", "Fear", "Happiness", "Sadness", "Surprise", "Neutral"] #(base)
list_emotions_text = ["Anger", "Disgust", "Fear", "Happiness", "Neutral", "Sadness", "Surprise"]
list_emotions_audio = ["Neutral", "Happiness", "Sadness", "Anger", "Fear", "Disgust", "Surprise"]

def organise_emotions_predicted(emotions_text, fromSource):

  if (fromSource == "text"):
    emotions_text[4], emotions_text[5] = emotions_text[5], emotions_text[4]
    emotions_text[5], emotions_text[6] = emotions_text[6], emotions_text[5]

  elif (fromSource == "audio"):
    emotions_text[0], emotions_text[3] = emotions_text[3], emotions_text[0]
    emotions_text[5], emotions_text[1] = emotions_text[1], emotions_text[5]
    emotions_text[2], emotions_text[4] = emotions_text[4], emotions_text[2]
    emotions_text[3], emotions_text[5] = emotions_text[5], emotions_text[3]
    emotions_text[5], emotions_text[6] = emotions_text[6], emotions_text[5]
  
  else:
    print("{} is not a valid source".format(fromSource))

  return emotions_text

# print(organise_emotions_predicted(list_emotions_text, fromSource="text"))

# print(list_emotions_image)

"""##Compute average results"""

def combine_data(filename, drawGraph):

  #array emotions
  emotions = ["Angry", "Disgust", "Fear", "Happiness", "Sad", "Surprise", "Neutral"]

  #Accuracy of each model
  text_accuracy = 0.8442
  audio_accuracy = 0.4371
  image_accuracy = 0.7042
  motion_accuracy = 0.6871
  global_accuracy = 0.7328

  #text
  result_text = organise_emotions_predicted(textPredict(speech_to_text(mp4_to_wav(filename)))[0], fromSource="text")
  print("Text prediction : " + str(emotions[np.where(result_text == np.max(result_text))[0][0]]) + " (accuracy: " + str(int(text_accuracy*100)) + "%)")

  #speech
  # result_audio = organise_emotions_predicted(audioPredict(filename)[0], fromSource="audio")
  # print("Audio prediction : " + str( emotions[np.where(result_audio == np.max(result_audio))[0][0]]) + " (accuracy: " + str(int(audio_accuracy*100)) + "%)")

  #image
  print("\nPROCESSING Face Emotion Recognition")
  result_image = imagePredict(filename, drawGraph)
  print("Image prediction : " + str(emotions[result_image.index(np.max(result_image))]) + " (accuracy: " + str(int(image_accuracy*100)) + "%)") #emotions[result_image.index(np.max(result_image))])


  #motion
  print("\nPROCESSING Motion Emotion Recognition")
  result_motion = motionPredict(filename, False) 
  print("\nMotion prediction : " + str(emotions[result_image.index(np.max(result_image))]) + " (accuracy: " + str(int(motion_accuracy*100)) + "%)") #emotions[result_image.index(np.max(result_image))])



    



  #Accuracy of each model for each emotion

  #Accuracy text
  text_fear_accuracy = 0.85
  text_sadness_accuracy = 0.87
  text_happiness_accuracy = 0.87
  text_anger_accuracy = 0.76
  text_disgust_accuracy = 0.96
  text_surprise_accuracy = 0.81
  text_neutral_accuracy = 0.80

  #Accuracy audio
  audio_fear_accuracy = 0.04
  audio_sadness_accuracy = 0.47
  audio_happiness_accuracy = 0.48
  audio_anger_accuracy = 0.54
  audio_disgust_accuracy = 0.40
  audio_surprise_accuracy = 0.38
  audio_neutral_accuracy = 0.75

  #Accuracy image
  image_fear_accuracy = 0.73
  image_sadness_accuracy = 0.57
  image_happiness_accuracy = 0.64
  image_anger_accuracy = 0.86
  image_disgust_accuracy = 0.79
  image_surprise_accuracy = 0.76
  image_neutral_accuracy = 0.58

  #Accuracy motion
  motion_fear_accuracy = 0.63
  motion_sadness_accuracy = 0.71
  motion_happiness_accuracy = 0.55
  motion_anger_accuracy = 0.79
  motion_disgust_accuracy = 0.73
  motion_surprise_accuracy = 0.55
  motion_neutral_accuracy = 0.85

  global_accuracy_fear_motion = motion_accuracy *  motion_fear_accuracy
  global_accuracy_sadness_motion = motion_accuracy *  motion_sadness_accuracy
  global_accuracy_happiness_motion = motion_accuracy *  motion_happiness_accuracy
  global_accuracy_anger_motion = motion_accuracy *  motion_anger_accuracy
  global_accuracy_disgust_motion = motion_accuracy *  motion_disgust_accuracy
  global_accuracy_surprise_motion = motion_accuracy *  motion_surprise_accuracy
  global_accuracy_neutral_motion = motion_accuracy *  motion_neutral_accuracy

  global_accuracy_fear_image = image_accuracy *  image_fear_accuracy
  global_accuracy_sadness_image = image_accuracy *  image_sadness_accuracy
  global_accuracy_happiness_image = image_accuracy *  image_happiness_accuracy
  global_accuracy_anger_image = image_accuracy *  image_anger_accuracy
  global_accuracy_disgust_image = image_accuracy *  image_disgust_accuracy
  global_accuracy_surprise_image = image_accuracy *  image_surprise_accuracy
  global_accuracy_neutral_image = image_accuracy *  image_neutral_accuracy

  global_accuracy_fear_audio = audio_accuracy *  audio_fear_accuracy
  global_accuracy_sadness_audio = audio_accuracy *  audio_sadness_accuracy
  global_accuracy_happiness_audio = audio_accuracy *  audio_happiness_accuracy
  global_accuracy_anger_audio = audio_accuracy *  audio_anger_accuracy
  global_accuracy_disgust_audio = audio_accuracy *  audio_disgust_accuracy
  global_accuracy_surprise_audio = audio_accuracy *  audio_surprise_accuracy
  global_accuracy_neutral_audio = audio_accuracy *  audio_neutral_accuracy

  global_accuracy_fear_text = text_accuracy *  text_fear_accuracy
  global_accuracy_sadness_text = text_accuracy *  text_sadness_accuracy
  global_accuracy_happiness_text = text_accuracy *  text_happiness_accuracy
  global_accuracy_anger_text = text_accuracy *  text_anger_accuracy
  global_accuracy_disgust_text = text_accuracy *  text_disgust_accuracy
  global_accuracy_surprise_text = text_accuracy *  text_surprise_accuracy
  global_accuracy_neutral_text = text_accuracy *  text_neutral_accuracy



  #Global prediction for each emotion
  global_fear_prediction = (result_motion[0] * global_accuracy_fear_motion + result_image[0] * global_accuracy_fear_image + result_audio[0] * global_accuracy_fear_audio  + result_text[0] * global_accuracy_fear_text) / (global_accuracy_fear_motion + global_accuracy_fear_image + global_accuracy_fear_audio +global_accuracy_fear_text)
  global_sadness_prediction =  (result_motion[1] * global_accuracy_sadness_motion + result_image[1] * global_accuracy_sadness_image + result_audio[1] * global_accuracy_sadness_audio  + result_text[1] * global_accuracy_sadness_text) / (global_accuracy_sadness_motion + global_accuracy_sadness_image + global_accuracy_sadness_audio +global_accuracy_sadness_text)
  global_happiness_prediction =  (result_motion[2] * global_accuracy_happiness_motion + result_image[2] * global_accuracy_happiness_image + result_audio[2] * global_accuracy_happiness_audio  + result_text[2] * global_accuracy_happiness_text) / (global_accuracy_happiness_motion + global_accuracy_happiness_image + global_accuracy_happiness_audio + global_accuracy_happiness_text)
  global_anger_prediction =  (result_motion[3] * global_accuracy_anger_motion + result_image[3] * global_accuracy_anger_image + result_audio[3] * global_accuracy_anger_audio  + result_text[3] * global_accuracy_anger_text) / (global_accuracy_anger_motion + global_accuracy_anger_image + global_accuracy_anger_audio + global_accuracy_anger_text)
  global_disgust_prediction =  (result_motion[4] * global_accuracy_disgust_motion + result_image[4] * global_accuracy_disgust_image + result_audio[4] * global_accuracy_disgust_audio  + result_text[4] * global_accuracy_disgust_text) / (global_accuracy_disgust_motion + global_accuracy_disgust_image + global_accuracy_disgust_audio + global_accuracy_disgust_text)
  global_surprise_prediction =  (result_motion[5] * global_accuracy_surprise_motion + result_image[5] * global_accuracy_surprise_image + result_audio[5] * global_accuracy_surprise_audio  + result_text[5] * global_accuracy_surprise_text) / (global_accuracy_surprise_motion + global_accuracy_surprise_image + global_accuracy_surprise_audio + global_accuracy_surprise_text)
  global_neutral_prediction =  (result_motion[6] * global_accuracy_neutral_motion + result_image[6] * global_accuracy_neutral_image + result_audio[6] * global_accuracy_neutral_audio  + result_text[6] * global_accuracy_neutral_text) / (global_accuracy_neutral_motion + global_accuracy_neutral_image + global_accuracy_neutral_audio + global_accuracy_neutral_text)


  list_global_predictions = [global_fear_prediction, global_sadness_prediction, global_happiness_prediction, global_anger_prediction, global_disgust_prediction, global_surprise_prediction, global_neutral_prediction]


  final_prediction = emotions[list_global_predictions.index(np.max(list_global_predictions))]

  print("FINAL PREDICTION : " +  str(final_prediction) + " (accuracy: " + str(int(global_accuracy*100)) + "%)") #emotions[result_image.index(np.max(result_image))]))


  # return final_prediction

"""#GLOBAL MAIN"""

# !ls
# filename = "anger eb.mov"
filename = "anger_luca.mp4"

combine_data(filename, drawGraph=True)



"""#LIVE PREDICTION

##Import for live
"""

# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time

"""##Functions for live"""

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

# initialize the Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))
  # get OpenCV format image
  img = js_to_image(data) 
  # grayscale img
  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  print(gray.shape)
  # get face bounding box coordinates using Haar Cascade
  faces = face_cascade.detectMultiScale(gray)
  # draw face bounding box on image
  for (x,y,w,h) in faces:
      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
  # save image
  cv2.imwrite(filename, img)

  return filename

# JavaScript to properly create our live video stream using our webcam as input
# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;
    
    var pendingResolve = null;
    var shutdown = false;
    
    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }
    
    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }
    
    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);
      
      const modelOut = document.createElement('div');
      modelOut.innerHTML = " ";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);
           
      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);
      
      const instruction = document.createElement('div');
      instruction.innerHTML = 
          '<span style="color: red; font-weight: bold;">';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };
      
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);
      
      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();
      
      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }
            
      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }
      
      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;
      
      return {'create': preShow - preCreate, 
              'show': preCapture - preShow, 
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)
  
def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

"""##Live"""

from keras.utils import img_to_array
import imutils
import cv2
from keras.models import load_model
import numpy as np
from google.colab.patches import cv2_imshow

# !ls

# parameters for loading data and images
detection_model_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
emotion_model_path = './_mini_XCEPTION.102-0.66.hdf5'

# hyper-parameters for bounding boxes shape
# loading models


face_detection = cv2.CascadeClassifier(detection_model_path)

emotion_classifier = load_model(emotion_model_path, compile=False)

EMOTIONS = ["Anger" ,"Disgust","Fear", "Happiness", "Sadness", "Surprise","Neutral"]

#feelings_faces = []
#for index, emotion in enumerate(EMOTIONS):
   # feelings_faces.append(cv2.imread('emojis/' + emotion + '.png', -1))

# starting video streaming

emotion_window = []

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
bbox = ''
count = 0 

while True:
    js_reply = video_frame(label_html, bbox)

    if not js_reply:
        break

    # convert JS response to OpenCV Image
    frame = js_to_image(js_reply["img"])
    #reading the frame
    frame = imutils.resize(frame,width=300)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)
    
    canvas = np.zeros((250, 300, 4), dtype="uint8")
    frameClone = frame.copy()
    if len(faces) > 0:
        faces = sorted(faces, reverse=True,
        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]
        (fX, fY, fW, fH) = faces
                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, and then prepare
            # the ROI for classification via the CNN
        roi = gray[fY:fY + fH, fX:fX + fW]
        roi = cv2.resize(roi, (64, 64))
        roi = roi.astype("float") / 255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi, axis=0)
        
        
        preds = emotion_classifier.predict(roi)[0]
        emotion_probability = np.max(preds)
        label = EMOTIONS[preds.argmax()]
        # print(label)

        canvas = cv2.rectangle(canvas,(fX,fY),(fX+fW,fY+fH),(0, 0, 255),2)
        canvas = cv2.putText(canvas, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)

    canvas[:,:,3] = (canvas.max(axis = 2) > 0 ).astype(int) * 255
    
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(canvas)
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes

"""# TEST INTERNSHIP"""

print("hello")